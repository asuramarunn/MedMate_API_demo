import os
import time
import tempfile
import numpy as np
import pandas as pd
import soundfile as sf
from pydub import AudioSegment
import torch
import jiwer
import subprocess
 

# -------------------------------
# 1Ô∏è‚É£ Folder ch·ª©a audio
# -------------------------------

AUDIO_DIR = "VOICE"  # <--- ƒë·ªïi th√†nh folder c·ªßa b·∫°n


from transformers import pipeline, WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor
from chunkformer import ChunkFormerModel


# ===============================
# 1Ô∏è‚É£ Load WHISPER fine-tuned (OFFLINE)
# ===============================

# üîπ ƒê∆∞·ªùng d·∫´n t·ªõi snapshot local c·ªßa Whisper fine-tuned
CHECKPOINT_LOCAL = "/home/datnguyen/.cache/huggingface/hub/models--asuramarunnn--medmate-whisper-tiny-vi-v1/snapshots/4707abe66416511895988dbc2f240eb0514b240b"

# üîπ ƒê∆∞·ªùng d·∫´n t·ªõi base model (n·∫øu c·∫ßn tokenizer/feature_extractor)
BASE_MODEL_ID = "/home/datnguyen/.cache/huggingface/hub/models--doof-ferb--whisper-tiny-vi/snapshots/a7f8c3da397f4d4b184b946f18647758682d6a05"

print("üîπ Loading Whisper fine-tuned (offline)...")
model = WhisperForConditionalGeneration.from_pretrained(CHECKPOINT_LOCAL)
tokenizer = WhisperTokenizer.from_pretrained(BASE_MODEL_ID, language="vi", task="transcribe")
feature_extractor = WhisperFeatureExtractor.from_pretrained(BASE_MODEL_ID)
device = 0 if torch.cuda.is_available() else -1

pipe_whisper = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=tokenizer,
    feature_extractor=feature_extractor,
    device=device,
)
PIPE_KWARGS = {"language": "vi", "task": "transcribe"}
print("‚úÖ Whisper ready.\n")

# ===============================
# 2Ô∏è‚É£ Load CHUNKFORMER (OFFLINE)
# ===============================

print("üîπ Loading Chunkformer (offline)...")

CHUNKFORMER_PATH = "/home/datnguyen/.cache/huggingface/hub/models--khanhld--chunkformer-ctc-large-vie/snapshots/311fc03558a895dc2b32957f2fb4236c7fb1455b"

chunkformer_model = ChunkFormerModel.from_pretrained(CHUNKFORMER_PATH)
print("‚úÖ Chunkformer ready.\n")

# ===============================
# 3Ô∏è‚É£ Load ZIPFORMER (sherpa_onnx)
# ===============================
print("üîπ Loading Zipformer...")
import sherpa_onnx


zipformer_model = sherpa_onnx.OfflineRecognizer.from_transducer(
    tokens="resources/config.json",
    encoder="resources/encoder-epoch-20-avg-10.onnx",
    decoder="resources/decoder-epoch-20-avg-10.onnx",
    joiner="resources/joiner-epoch-20-avg-10.onnx",
    num_threads=4,
    decoding_method="greedy_search",
)

print("‚úÖ Zipformer ready.\n")



# -------------------------------
# 5Ô∏è‚É£ Utility functions
# -------------------------------
def get_audio_path(audio_name):
    path = os.path.join(AUDIO_DIR, f"{audio_name}.m4a")
    if not os.path.exists(path):
        raise FileNotFoundError(f"File {path} kh√¥ng t·ªìn t·∫°i")
    return path

def convert_to_wav(input_data):
    """
    Nh·∫≠n:
      - tuple (sr, np.array) t·ª´ microphone
      - ho·∫∑c ƒë∆∞·ªùng d·∫´n file (.mp3, .mp4, .m4a, .wav, ...)
    Tr·∫£ v·ªÅ:
      - tuple (sr, np.array)
    """
    if isinstance(input_data, tuple):
        # D·ªØ li·ªáu t·ª´ mic: (sr, np.array)
        return input_data

    elif isinstance(input_data, str) and os.path.exists(input_data):
        # D·ªØ li·ªáu t·ª´ file
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_wav_path = tmp_wav.name

        # Chuy·ªÉn m·ªçi ƒë·ªãnh d·∫°ng √¢m thanh v·ªÅ mono 16kHz WAV
        command = [
            "ffmpeg", "-y",
            "-i", input_data,
            "-ar", "16000",  # sample rate
            "-ac", "1",      # mono
            "-f", "wav", tmp_wav_path,
        ]
        subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # ƒê·ªçc d·ªØ li·ªáu WAV v·ª´a convert
        audio_data, sr = sf.read(tmp_wav_path)
        os.remove(tmp_wav_path)
        return sr, audio_data

    else:
        raise ValueError(f"ƒê·∫ßu v√†o √¢m thanh kh√¥ng h·ª£p l·ªá: {input_data}")

def load_audio(audio_input):
    """
    H√†m ƒë·ªçc √¢m thanh v√† tr·∫£ v·ªÅ:
      - data: numpy array (mono)
      - sr: sample rate
      - duration: ƒë·ªô d√†i t√≠nh b·∫±ng gi√¢y
    """
    sr, data = convert_to_wav(audio_input)

    # N·∫øu multi-channel (trong tr∆∞·ªùng h·ª£p l·ªói), chuy·ªÉn v·ªÅ mono
    if len(data.shape) > 1:
        data = np.mean(data, axis=1)

    duration = len(data) / sr
    return data, sr, duration


# ---------------- Normalization + WER helpers ----------------
import re, unicodedata, difflib, jiwer

# -------- B·∫¢NG C∆† S·ªû -------- #
_DIGIT_WORD = {"0":"kh√¥ng","1":"m·ªôt","2":"hai","3":"ba","4":"b·ªën","5":"nƒÉm","6":"s√°u","7":"b·∫£y","8":"t√°m","9":"ch√≠n"}
_NUM_WORDS_SIMPLE = {
    0:"kh√¥ng",1:"m·ªôt",2:"hai",3:"ba",4:"b·ªën",5:"nƒÉm",6:"s√°u",7:"b·∫£y",8:"t√°m",9:"ch√≠n",
    10:"m∆∞·ªùi",11:"m∆∞·ªùi m·ªôt",12:"m∆∞·ªùi hai",13:"m∆∞·ªùi ba",14:"m∆∞·ªùi b·ªën",15:"m∆∞·ªùi lƒÉm",
    20:"hai m∆∞∆°i",30:"ba m∆∞∆°i",40:"b·ªën m∆∞∆°i",50:"nƒÉm m∆∞∆°i",60:"s√°u m∆∞∆°i",
    70:"b·∫£y m∆∞∆°i",80:"t√°m m∆∞∆°i",90:"ch√≠n m∆∞∆°i"
}

def int_to_vn(n:int)->str:
    if n<0: return "√¢m "+int_to_vn(-n)
    if n<=20: return _NUM_WORDS_SIMPLE.get(n,str(n))
    if n<100:
        tens=(n//10)*10; unit=n%10
        return _NUM_WORDS_SIMPLE[tens]+("" if unit==0 else " "+_NUM_WORDS_SIMPLE.get(unit,str(unit)))
    if n<1000:
        h=n//100; rest=n%100
        return _NUM_WORDS_SIMPLE[h]+" trƒÉm"+("" if rest==0 else " "+int_to_vn(rest))
    if n<10000:
        th=n//1000; rest=n%1000
        return _NUM_WORDS_SIMPLE[th]+" ngh√¨n"+("" if rest==0 else " "+int_to_vn(rest))
    return str(n)

def frac_digits_to_vn(frac:str)->str:
    return " ".join(_DIGIT_WORD.get(ch,ch) for ch in frac)

def number_to_vn_words(token:str)->str:
    s=token.replace(",",".")
    if "." in s:
        ip,fp=s.split(".",1)
        try: ipw=int_to_vn(int(ip))
        except: ipw=" ".join(_DIGIT_WORD.get(ch,ch) for ch in ip)
        fpw=frac_digits_to_vn(fp)
        return f"{ipw} ch·∫•m {fpw}"
    try: return int_to_vn(int(s))
    except: return " ".join(_DIGIT_WORD.get(ch,ch) for ch in s)

# -------- ƒê∆†N V·ªä & THU·∫¨T NG·ªÆ Y H·ªåC -------- #
_UNIT_PATTERNS = [
    (r"\bmmhg\b","milimet th·ªßy ng√¢n"),
    (r"\bspo2\b","sp √¥ hai"),
    (r"\bbmi\b","b√™ em ai"),
    (r"¬∞c","ƒë·ªô c"),
    (r"kg\/m2","kg tr√™n m√©t vu√¥ng"),
    (r"kg\/m¬≤","kg tr√™n m√©t vu√¥ng"),
    (r"%","ph·∫ßn trƒÉm"),
    (r"/ph√∫t","tr√™n ph√∫t"),
    (r"\/","tr√™n"),
]

_SPECIAL_TERMS = {
    "m·∫°ch":"mach",
    "huy·∫øt √°p":"huyet ap",
    "nhi·ªát ƒë·ªô":"nhiet do",
    "nh·ªãp th·ªü":"nhip tho",
    "t·ªânh":"tinh",
    "to√†n tr·∫°ng":"toan trang"
}

# -------- NORMALIZATION -------- #
_PUNCT_RE = re.compile(r"[^0-9a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá"
                       r"√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±"
                       r"·ª≥√Ω·ª∑·ªπ·ªµƒë\s¬∞¬≤]", re.U)

def normalize_text(text:str)->str:
    if text is None: return ""
    t=str(text).strip().lower()
    t=unicodedata.normalize("NFC",t)
    
    # thay th·∫ø thu·∫≠t ng·ªØ y h·ªçc tr∆∞·ªõc
    for k,v in _SPECIAL_TERMS.items():
        t=re.sub(rf"\b{k}\b",v,t)
    
    # x·ª≠ l√Ω t·ª´ "ph·∫©y", "ch·∫•m"
    t=re.sub(r"\b(ph·∫©y|ch·∫•m)\b","ch·∫•m",t)
    
    # thay ƒë∆°n v·ªã & k√Ω hi·ªáu
    for pat,rep in _UNIT_PATTERNS:
        t=re.sub(pat, f" {rep} ", t, flags=re.I)
    
    # t√°ch 100/60 ƒë·ªÉ parse ch√≠nh x√°c
    t=re.sub(r"(\d+)/(\d+)", r"\1 / \2", t)
    
    # thay s·ªë th√†nh ch·ªØ
    t=re.sub(r"\d+[0-9.,]*", lambda m:number_to_vn_words(m.group(0)), t)
    
    # chu·∫©n ho√° s·ªë ƒë·ªçc ‚ÄúnƒÉm/lƒÉm‚Äù
    t=re.sub(r"\b(lƒÉm|nƒÉm)\b","nƒÉm",t)
    
    # xo√° d·∫•u
    t=_PUNCT_RE.sub(" ",t)
    t=re.sub(r"\s+"," ",t).strip()
    return t

# -------- WER & SAI KH√ÅC -------- #
def compute_normalized_wer(ref:str, hyp:str)->float:
    r=normalize_text(ref)
    h=normalize_text(hyp)
    return float(jiwer.wer(r,h))

def get_wrong_pairs(ref:str,hyp:str,max_pairs=10000)->str:
    r_words=normalize_text(ref).split()
    h_words=normalize_text(hyp).split()
    sm=difflib.SequenceMatcher(a=r_words,b=h_words)
    pairs=[]
    for tag,i1,i2,j1,j2 in sm.get_opcodes():
        if tag=="equal": continue
        if tag=="replace":
            for a,b in zip(r_words[i1:i2],h_words[j1:j2]):
                # n·∫øu kh√°c d·∫°ng s·ªë m√† t∆∞∆°ng ƒë∆∞∆°ng (vd: 5.2 vs 5,2)
                if re.sub(r"[.,]","",a)==re.sub(r"[.,]","",b): continue
                pairs.append(f"{a}->{b}")
        elif tag=="delete":
            for a in r_words[i1:i2]: pairs.append(f"{a}->")
        elif tag=="insert":
            for b in h_words[j1:j2]: pairs.append(f"->{b}")
        if len(pairs)>=max_pairs: break
    return ", ".join(pairs)


# -------------------------------
# 6Ô∏è‚É£ Transcribe functions
# -------------------------------
import os
import time
import tempfile
import numpy as np
import soundfile as sf
import jiwer

# ===============================
# 1Ô∏è‚É£ Whisper
# ===============================
def transcribe_whisper(audio_array, sr, max_chunk_duration=25):
    import numpy as np
    start = time.time()

    chunk_size = int(sr * max_chunk_duration)
    num_chunks = int(np.ceil(len(audio_array) / chunk_size))

    full_text = ""
    for i in range(num_chunks):
        chunk = audio_array[i * chunk_size:(i + 1) * chunk_size]
        result = pipe_whisper(
            {"array": chunk, "sampling_rate": sr},
            generate_kwargs=PIPE_KWARGS,
            return_timestamps=False,
        )
        full_text += " " + result.get("text", "").strip()

    elapsed = time.time() - start
    return full_text.strip(), elapsed


# ===============================
# 2Ô∏è‚É£ Chunkformer
# ===============================
def transcribe_chunkformer(audio_array, sr):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
        sf.write(tmp_wav.name, audio_array, sr)
        tmp_path = tmp_wav.name

    start = time.time()
    result = chunkformer_model.endless_decode(
        audio_path=tmp_path,
        chunk_size=64,
        left_context_size=128,
        right_context_size=128,
        return_timestamps=False,
    )
    elapsed = time.time() - start

    text = result["text"] if isinstance(result, dict) else str(result)
    os.remove(tmp_path)
    return text.strip(), elapsed


# ===============================
# 3Ô∏è‚É£ Zipformer
# ===============================
def transcribe_zipformer(audio_array, sr):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
        sf.write(tmp_wav.name, audio_array, sr)
        tmp_path = tmp_wav.name

    try:
        audio, sr = sf.read(tmp_path)
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)
        audio = audio.astype(np.float32)

        start = time.time()
        stream = zipformer_model.create_stream()
        stream.accept_waveform(sr, audio)
        zipformer_model.decode_stream(stream)
        text = stream.result.text
        elapsed = time.time() - start
        os.remove(tmp_path)

        return text.strip(), elapsed
    except Exception as e:
        os.remove(tmp_path)
        return f"‚ùå L·ªói khi x·ª≠ l√Ω Zipformer: {e}", 0.0


# ===============================
# 4Ô∏è‚É£ X·ª≠ l√Ω t·ª´ng h√†ng CSV
# ===============================
def process_row(row):
    audio_name = row["Audio"]
    ref_text = row["Transcribe"]
    results = []

    try:
        # L·∫•y ƒë∆∞·ªùng d·∫´n audio
        audio_file = get_audio_path(audio_name)
        sr, audio_array = convert_to_wav(audio_file)

        # T√≠nh ƒë·ªô d√†i
        duration = len(audio_array) / sr

        # -------------------------------
        # Whisper
        # -------------------------------
        text, t = transcribe_whisper(audio_array, sr)
        results.append({
            "audio": audio_name,
            "audio_length": duration,
            "transcribe": text,
            "wer": compute_normalized_wer(ref_text, text),
            "speed": t,
            "wrong": get_wrong_pairs(ref_text, text),
            "model": "Whisper"
        })

        # -------------------------------
        # ChunkFormer
        # -------------------------------
        text, t = transcribe_chunkformer(audio_array, sr)
        results.append({
            "audio": audio_name,
            "audio_length": duration,
            "transcribe": text,
            "wer": compute_normalized_wer(ref_text, text),
            "speed": t,
            "wrong": get_wrong_pairs(ref_text, text),
            "model": "ChunkFormer"
        })

        # -------------------------------
        # Zipformer
        # -------------------------------
        text, t = transcribe_zipformer(audio_array, sr)
        results.append({
            "audio": audio_name,
            "audio_length": duration,
            "transcribe": text,
            "wer": compute_normalized_wer(ref_text, text),
            "speed": t,
            "wrong": get_wrong_pairs(ref_text, text),
            "model": "Zipformer"
        })

    except Exception as e:
        print(f"‚ùå Error processing {audio_name}: {e}")

    return results

# -------------------------------
# 8Ô∏è‚É£ Main
# -------------------------------
if __name__ == "__main__":
    df = pd.read_csv("audio.csv")
    print(f"‚úÖ Load CSV v·ªõi {len(df)} h√†ng.")

    all_results = []
    for row in df.to_dict(orient="records"):
        all_results.extend(process_row(row))

    df_result = pd.DataFrame(all_results)
    df_result.to_csv("results.csv", index=False)
    print("‚úÖ ƒê√£ ghi k·∫øt qu·∫£ ra results.csv")
