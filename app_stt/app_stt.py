# import os
# import tempfile
# import gradio as gr
# import torch
# import subprocess
# import numpy as np
# import soundfile as sf
# import time

# from transformers import (
#     pipeline,
#     WhisperTokenizer,
#     WhisperFeatureExtractor,
#     WhisperForConditionalGeneration,
# )
# from huggingface_hub import hf_hub_download

# from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor, pipeline
# import torch
# from chunkformer import ChunkFormerModel

# # ===============================
# # 1Ô∏è‚É£ Load WHISPER fine-tuned (OFFLINE)
# # ===============================

# # üîπ ƒê∆∞·ªùng d·∫´n t·ªõi snapshot local c·ªßa Whisper fine-tuned
# CHECKPOINT_LOCAL = "/home/datnguyen/.cache/huggingface/hub/models--asuramarunnn--medmate-whisper-tiny-vi-v1/snapshots/4707abe66416511895988dbc2f240eb0514b240b"

# # üîπ ƒê∆∞·ªùng d·∫´n t·ªõi base model (n·∫øu c·∫ßn tokenizer/feature_extractor)
# BASE_MODEL_ID = "/home/datnguyen/.cache/huggingface/hub/models--doof-ferb--whisper-tiny-vi/snapshots/a7f8c3da397f4d4b184b946f18647758682d6a05"

# print("üîπ Loading Whisper fine-tuned (offline)...")
# model = WhisperForConditionalGeneration.from_pretrained(CHECKPOINT_LOCAL)
# tokenizer = WhisperTokenizer.from_pretrained(BASE_MODEL_ID, language="vi", task="transcribe")
# feature_extractor = WhisperFeatureExtractor.from_pretrained(BASE_MODEL_ID)
# device = 0 if torch.cuda.is_available() else -1

# pipe_whisper = pipeline(
#     "automatic-speech-recognition",
#     model=model,
#     tokenizer=tokenizer,
#     feature_extractor=feature_extractor,
#     device=device,
# )
# PIPE_KWARGS = {"language": "vi", "task": "transcribe"}
# print("‚úÖ Whisper ready.\n")

# # ===============================
# # 2Ô∏è‚É£ Load CHUNKFORMER (OFFLINE)
# # ===============================

# print("üîπ Loading Chunkformer (offline)...")

# CHUNKFORMER_PATH = "/home/datnguyen/.cache/huggingface/hub/models--khanhld--chunkformer-ctc-large-vie/snapshots/311fc03558a895dc2b32957f2fb4236c7fb1455b"

# chunkformer_model = ChunkFormerModel.from_pretrained(CHUNKFORMER_PATH)
# print("‚úÖ Chunkformer ready.\n")

# # ===============================
# # 3Ô∏è‚É£ Load ZIPFORMER (sherpa_onnx)
# # ===============================
# print("üîπ Loading Zipformer...")
# import sherpa_onnx


# zipformer_model = sherpa_onnx.OfflineRecognizer.from_transducer(
#     tokens="resources/config.json",
#     encoder="resources/encoder-epoch-20-avg-10.onnx",
#     decoder="resources/decoder-epoch-20-avg-10.onnx",
#     joiner="resources/joiner-epoch-20-avg-10.onnx",
#     num_threads=4,
#     decoding_method="greedy_search",
# )

# print("‚úÖ Zipformer ready.\n")


# # ===============================
# # 4Ô∏è‚É£ Ti·ªán √≠ch chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng
# # ===============================
# def convert_to_wav(input_data):
#     """
#     Nh·∫≠n:
#       - tuple (sr, np.array) t·ª´ microphone
#       - ho·∫∑c ƒë∆∞·ªùng d·∫´n file upload (.mp3, .mp4, .m4a, .wav, ...)
#     Tr·∫£ v·ªÅ:
#       - tuple (sr, np.array)
#     """
#     if isinstance(input_data, tuple):
#         return input_data

#     elif isinstance(input_data, str) and os.path.exists(input_data):
#         with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
#             tmp_wav_path = tmp_wav.name
#         command = [
#             "ffmpeg", "-y", "-i", input_data,
#             "-ar", "16000", "-ac", "1", "-f", "wav", tmp_wav_path,
#         ]
#         subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
#         audio_data, sr = sf.read(tmp_wav_path)
#         os.remove(tmp_wav_path)
#         return sr, audio_data
#     else:
#         raise ValueError("D·ªØ li·ªáu √¢m thanh kh√¥ng h·ª£p l·ªá.")


# # ===============================
# # 5Ô∏è‚É£ H√†m chia audio
# # ===============================
# def chunk_audio(audio_array, sr, max_duration=25):
#     """Chia audio th√†nh c√°c ƒëo·∫°n ~25s"""
#     chunk_size = int(sr * max_duration)
#     num_chunks = int(np.ceil(len(audio_array) / chunk_size))
#     return [audio_array[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks)]


# import sherpa_onnx
# import soundfile as sf
# import numpy as np

# def transcribe_zipformer(audio_path: str):
#     try:
#         # ƒê·ªçc audio (mono, float32)
#         audio, sr = sf.read(audio_path)
#         if len(audio.shape) > 1:
#             audio = np.mean(audio, axis=1)  # chuy·ªÉn stereo ‚Üí mono
#         audio = audio.astype(np.float32)

#         # T·∫°o stream m·ªõi
#         stream = zipformer_model.create_stream()

#         # Th√™m waveform v√†o stream
#         stream.accept_waveform(sr, audio)

#         # Decode
#         zipformer_model.decode_stream(stream)

#         # L·∫•y k·∫øt qu·∫£
#         text = stream.result.text
#         return text.strip()

#     except Exception as e:
#         return f"‚ùå L·ªói khi x·ª≠ l√Ω Zipformer: {e}"

# # ===============================
# # 6Ô∏è‚É£ H√†m inference cho t·ª´ng model
# # ===============================
# def transcribe(audio_input, model_name):
#     if audio_input is None:
#         return "‚ö†Ô∏è Vui l√≤ng ghi √¢m ho·∫∑c upload file √¢m thanh / video."

#     try:
#         sr, audio_array = convert_to_wav(audio_input)

#         # ===============================
#         # Whisper
#         # ===============================
#         if model_name == "Whisper (fine-tuned)":
#             chunks = chunk_audio(np.array(audio_array), sr)
#             full_text = ""
#             t0 = time.time()
#             for i, chunk in enumerate(chunks):
#                 result = pipe_whisper(
#                     {"array": chunk, "sampling_rate": sr},
#                     generate_kwargs=PIPE_KWARGS,
#                     return_timestamps=False,
#                 )
#                 full_text += " " + result.get("text", "").strip()
#             elapsed = time.time() - t0
#             return f"üïí {elapsed:.2f}s\n\n{full_text.strip()}"

#         # ===============================
#         # Chunkformer
#         # ===============================
#         elif model_name == "Chunkformer":
#             with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
#                 sf.write(tmp_wav.name, audio_array, sr)
#                 t0 = time.time()
#                 result = chunkformer_model.endless_decode(
#                     audio_path=tmp_wav.name,
#                     chunk_size=64,
#                     left_context_size=128,
#                     right_context_size=128,
#                     return_timestamps=False,
#                 )
#                 elapsed = time.time() - t0
#                 text = result["text"] if isinstance(result, dict) else str(result)
#             return f"üïí {elapsed:.2f}s\n\n{text.strip()}"

#         # ===============================
#         # Zipformer
#         # ===============================
#         elif model_name == "Zipformer":
#             with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
#                 sf.write(tmp_wav.name, audio_array, sr)
#                 t0 = time.time()
#                 result = transcribe_zipformer(tmp_wav.name)
#                 elapsed = time.time() - t0
#             return f"üïí {elapsed:.2f}s\n\n{result.strip()}"

#         else:
#             return "‚ùå Model kh√¥ng h·ª£p l·ªá."

#     except Exception as e:
#         return f"‚ùå L·ªói khi x·ª≠ l√Ω √¢m thanh: {e}"


# # ===============================
# # 7Ô∏è‚É£ Gradio UI
# # ===============================
# demo = gr.Interface(
#     fn=transcribe,
#     inputs=[
#         gr.Audio(
#             sources=["microphone", "upload"],
#             type="filepath",
#             label="üé§ Ghi √¢m ho·∫∑c t·∫£i l√™n file (.mp3, .mp4, .m4a, .wav, ...)",
#         ),
#         gr.Dropdown(
#             ["Whisper (fine-tuned)", "Chunkformer", "Zipformer"],
#             value="Whisper (fine-tuned)",
#             label="Ch·ªçn m√¥ h√¨nh STT",
#         ),
#     ],
#     outputs=gr.Textbox(label="üìù K·∫øt qu·∫£ chuy·ªÉn √¢m", lines=8),
#     title="üáªüá≥ Speech-to-Text Demo (Vietnamese ASR)",
#     description=(
#         "So s√°nh 3 m√¥ h√¨nh STT ti·∫øng Vi·ªát:\n"
#         "‚Ä¢ Whisper fine-tuned (MedMate)\n"
#         "‚Ä¢ Chunkformer (CTC)\n"
#         "‚Ä¢ Zipformer (RNNT)\n\n"
#         "H·ªó tr·ª£ upload ho·∫∑c ghi √¢m tr·ª±c ti·∫øp."
#     ),
# )

# if __name__ == "__main__":
#     demo.launch(server_name="0.0.0.0", server_port=7860)


# ------------------------------------------------------------------------------------

import os
import tempfile
import gradio as gr
import torch
import subprocess
import numpy as np
import soundfile as sf
import time
from dotenv import load_dotenv
from tqdm import tqdm
import google.generativeai as genai

from transformers import (
    pipeline,
    WhisperTokenizer,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
)
# from chunkformer import ChunkFormerModel
import sherpa_onnx
from chunkformer import ChunkFormerModel


# ===============================
# 0Ô∏è‚É£ Load .env & Gemini config
# ===============================
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y GEMINI_API_KEY trong .env")

genai.configure(api_key=API_KEY)
GEMINI_MODEL = "gemini-2.0-flash"
gemini_model = genai.GenerativeModel(GEMINI_MODEL)

SYSTEM_PROMPT = """
B·∫°n l√† b√°c sƒ© Vi·ªát Nam chuy√™n v·ªÅ hi·ªáu ƒë√≠nh vƒÉn b·∫£n y khoa nh·∫≠n d·∫°ng t·ª´ gi·ªçng n√≥i.
Nhi·ªám v·ª• c·ªßa b·∫°n:
1. Chu·∫©n h√≥a ch√≠nh t·∫£, d·∫•u c√¢u, vi·∫øt hoa.
2. Hi·ªáu ch·ªânh c√°c thu·∫≠t ng·ªØ y khoa sai (v√≠ d·ª• ‚Äúsp hai ch√≠n lƒÉm‚Äù ‚Üí ‚ÄúSpO2 95%‚Äù, ‚Äúmilimet th·ªßy ng√¢n‚Äù ‚Üí ‚ÄúmmHg‚Äù).
3. Vi·∫øt l·∫°i vƒÉn b·∫£n y khoa chu·∫©n, m·∫°ch l·∫°c, ƒë√∫ng ng·ªØ ph√°p, gi·ªØ nguy√™n n·ªôi dung.
4. Kh√¥ng th√™m di·ªÖn gi·∫£i, ch·ªâ tr·∫£ v·ªÅ vƒÉn b·∫£n ƒë√£ ch·ªânh s·ª≠a.
5. X√≥a c√°c t·ª´ th·ª´a, l·∫∑p t·ª´, c√¢u v√¥ nghƒ©a.
6. Gi·ªØ nguy√™n c√°c con s·ªë, ƒë∆°n v·ªã ƒëo l∆∞·ªùng.
"""

# ===============================
# 2Ô∏è‚É£ Load CHUNKFORMER (OFFLINE)
# ===============================

print("üîπ Loading Chunkformer (offline)...")

CHUNKFORMER_PATH = "/home/datnguyen/.cache/huggingface/hub/models--khanhld--chunkformer-ctc-large-vie/snapshots/311fc03558a895dc2b32957f2fb4236c7fb1455b"

chunkformer_model = ChunkFormerModel.from_pretrained(CHUNKFORMER_PATH)
print("‚úÖ Chunkformer ready.\n")

# ===============================
# 3Ô∏è‚É£ Load ZIPFORMER (sherpa_onnx)
# ===============================
print("üîπ Loading Zipformer...")
zipformer_model = sherpa_onnx.OfflineRecognizer.from_transducer(
    tokens="resources/config.json",
    encoder="resources/encoder-epoch-20-avg-10.onnx",
    decoder="resources/decoder-epoch-20-avg-10.onnx",
    joiner="resources/joiner-epoch-20-avg-10.onnx",
    num_threads=4,
    decoding_method="greedy_search",
)
print("‚úÖ Zipformer ready.\n")


# ===============================
# 4Ô∏è‚É£ Convert / Chunk utilities
# ===============================
def convert_to_wav(input_data):
    if isinstance(input_data, tuple):
        return input_data
    elif isinstance(input_data, str) and os.path.exists(input_data):
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
            tmp_wav_path = tmp_wav.name
        command = [
            "ffmpeg", "-y", "-i", input_data,
            "-ar", "16000", "-ac", "1", "-f", "wav", tmp_wav_path,
        ]
        subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        audio_data, sr = sf.read(tmp_wav_path)
        os.remove(tmp_wav_path)
        return sr, audio_data
    else:
        raise ValueError("D·ªØ li·ªáu √¢m thanh kh√¥ng h·ª£p l·ªá.")


def chunk_audio(audio_array, sr, max_duration=25):
    chunk_size = int(sr * max_duration)
    num_chunks = int(np.ceil(len(audio_array) / chunk_size))
    return [audio_array[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks)]


def transcribe_zipformer(audio_path: str):
    try:
        audio, sr = sf.read(audio_path)
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)
        audio = audio.astype(np.float32)
        stream = zipformer_model.create_stream()
        stream.accept_waveform(sr, audio)
        zipformer_model.decode_stream(stream)
        text = stream.result.text
        return text.strip()
    except Exception as e:
        return f"‚ùå L·ªói khi x·ª≠ l√Ω Zipformer: {e}"


# ===============================
# 5Ô∏è‚É£ Auto Refiner (Gemini)
# ===============================
def refine_text(raw_text):
    try:
        if not raw_text or raw_text.strip() == "":
            return "‚ö†Ô∏è Kh√¥ng c√≥ vƒÉn b·∫£n ƒë·ªÉ hi·ªáu ƒë√≠nh."
        prompt = f"{SYSTEM_PROMPT}\n\nVƒÉn b·∫£n c·∫ßn hi·ªáu ch·ªânh:\n{raw_text}\n\nK·∫øt qu·∫£ ch·ªânh s·ª≠a:"
        response = gemini_model.generate_content(prompt)
        refined = ""
        if hasattr(response, "text") and response.text:
            refined = response.text.strip()
        elif response.candidates and response.candidates[0].content.parts:
            refined = response.candidates[0].content.parts[0].text.strip()
        return refined or raw_text
    except Exception as e:
        return f"‚ùå L·ªói khi hi·ªáu ƒë√≠nh: {e}"


# ===============================
# 6Ô∏è‚É£ Inference
# ===============================
def transcribe(audio_input, model_name):
    if audio_input is None:
        return "‚ö†Ô∏è Vui l√≤ng ghi √¢m ho·∫∑c upload file √¢m thanh / video."

    try:
        sr, audio_array = convert_to_wav(audio_input)


        if model_name == "Chunkformer":
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
                sf.write(tmp_wav.name, audio_array, sr)
                t0 = time.time()
                result = chunkformer_model.endless_decode(
                    audio_path=tmp_wav.name,
                    chunk_size=64,
                    left_context_size=128,
                    right_context_size=128,
                    return_timestamps=False,
                )
                elapsed = time.time() - t0
                text = result["text"] if isinstance(result, dict) else str(result)
            refined = refine_text(text.strip())
            return f"üïí {elapsed:.2f}s\n\n---\n**Raw:** {text.strip()}\n\n---\n**Refined:** {refined}"


        elif model_name == "Zipformer":
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
                sf.write(tmp_wav.name, audio_array, sr)
                t0 = time.time()
                result = transcribe_zipformer(tmp_wav.name)
                elapsed = time.time() - t0
            refined = refine_text(result.strip())
            return f"üïí {elapsed:.2f}s\n\n---\n**Raw:** {result.strip()}\n\n---\n**Refined:** {refined}"

        else:
            return "‚ùå Model kh√¥ng h·ª£p l·ªá."

    except Exception as e:
        return f"‚ùå L·ªói khi x·ª≠ l√Ω √¢m thanh: {e}"


# ===============================
# 7Ô∏è‚É£ Gradio UI
# ===============================
demo = gr.Interface(
    fn=transcribe,
    inputs=[
        gr.Audio(
            sources=["microphone", "upload"],
            type="filepath",
            label="üé§ Ghi √¢m ho·∫∑c t·∫£i l√™n file (.mp3, .mp4, .m4a, .wav, ...)",
        ),
        gr.Dropdown(
            [ "Chunkformer", "Zipformer"],
            value="Chunkformer",
            label="Ch·ªçn m√¥ h√¨nh STT",
        ),
    ],
    outputs=gr.Markdown(label="üìù K·∫øt qu·∫£ chuy·ªÉn √¢m"),
    title="üáªüá≥ Speech-to-Text Demo + Auto Refiner (Vietnamese ASR)",
    description=(
        "So s√°nh 3 m√¥ h√¨nh STT ti·∫øng Vi·ªát (Whisper, Chunkformer, Zipformer)\n\n"
        "Sau khi nh·∫≠n d·∫°ng, Gemini s·∫Ω t·ª± ƒë·ªông hi·ªáu ƒë√≠nh vƒÉn b·∫£n y khoa ‚ú®"
    ),
)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
